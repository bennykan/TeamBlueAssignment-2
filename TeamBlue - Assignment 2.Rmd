---
title: "Clustering Algorithm for Unupervised Learning - Credit Card Client Anomaly Analysis"
author: "Tyler Blakeley, Benjamin Kan, Mohammad Islam, Avijeet Singh"
date: "October 29 2018"
output:
  html_document:
    fig_height: 4.5
    fig_width: 7
    highlight: tango
    number_sections: yes
    theme: readable
    toc: yes
  pdf_document:
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE)
```
# Business Understanding
We work at the Retail Credit Risk Analytics department at the Bank of Taiwan. Recently, there have been increasing credit card debt defaults in our bank. Senior Management would like our department to develop a machine learning algorithm to find anomalies in the data that we hope will show early warning signs of default. This will allow the Retail Credit Risk and Collections departments to act early by reducing these cleints' credit card limits to minimize the losses. We would also like to find out which demographics are in the anomaly group which would indicate high susceptiblility of defaults. The Management instructed us to use data from the third parties to build the algorithms as proof-of-concepts before we use our own data. They would also like us to build a user-friendly app to allow them to load in the dataset and identify clients that are in the anomaly group which may indicate high risk of defaulting on their credit card debts.        

# Data Understanding

## Data Source and Collection
We sourced the third party credit card data from Kaggle (https://www.kaggle.com/uciml/default-of-credit-card-clients-dataset). This dataset contains information on default payments, demographic factors, credit data, history of payment, and bill statements of credit card clients in Taiwan from April 2005 to September 2005. As mentioned above, the goal is to identify a group of customers who have high default risks. 

## Data Description
In the dataset, there are 25 variables:

Variable Name              | Description
---------------------------|--------------------------------------------------------------------------------
ID                         | ID of each client
LIMIT_BAL                  | Amount of given credit in NT dollars (includes individual and                                               | family/supplementary credit
SEX                        | Gender (1=male, 2=female)
EDUCATION                  | (1=graduate school, 2=university, 3=high school, 4=others, 5=unknown, 6=unknown)
MARRIAGE                   | Marital status (1=married, 2=single, 3=others)
AGE                        | Age in years
PAY_0                      | Repayment status in September, 2005 (-1=pay duly, 1=payment delay for one month,                            | 2=payment delay for two months, ... 8=payment delay for eight months, 9=payment                             | delay fornine months and above)                 
PAY_2                      | Repayment status in August, 2005 (scale same as above)
PAY_3                      | Repayment status in July, 2005 (scale same as above)
PAY_4                      | Repayment status in June, 2005 (scale same as above)
PAY_5                      | Repayment status in May, 2005 (scale same as above)
PAY_6                      | Repayment status in April, 2005 (scale same as above)
BILL_AMT1                  | Amount of bill statement in September, 2005 (NT dollar)
BILL_AMT2                  | Amount of bill statement in August, 2005 (NT dollar)
BILL_AMT3                  | Amount of bill statement in July, 2005 (NT dollar)
BILL_AMT4                  | Amount of bill statement in June, 2005 (NT dollar)
BILL_AMT5                  | Amount of bill statement in May, 2005 (NT dollar)
BILL_AMT6                  | Amount of bill statement in April, 2005 (NT dollar)
PAY_AMT1                   | Amount of previous payment in September, 2005 (NT dollar)
PAY_AMT2                   | Amount of previous payment in August, 2005 (NT dollar)
PAY_AMT3                   | Amount of previous payment in July, 2005 (NT dollar)
PAY_AMT4                   | Amount of previous payment in June, 2005 (NT dollar)
PAY_AMT5                   | Amount of previous payment in May, 2005 (NT dollar)
PAY_AMT6                   | Amount of previous payment in April, 2005 (NT dollar)
default.payment.next.month | Default payment (1=yes, 0=no)



## Data Exploration

### Load Packages
```{r, message=FALSE,warning=FALSE}
#import packages;
library(dplyr)
library(reshape2)
library(ggplot2)
library(Hmisc)
library(corrplot)
library(mice)
library(VIM)
library(pROC)
library(caret)
library(corrgram)
library(GGally)
library(ggthemes) 
library(DMwR)
library(gridExtra)
library(rattle)
library(readxl)
library(cluster)
```
### Load Datasets
Now that the packages are loaded,we can load in the dataset.

```{r, message=FALSE}
data <- read.csv("default of credit card clients.csv", header = TRUE, na= 'NA')
```

Now lets look at the structure of the data.

```{r, message=FALSE}
str(data)
```
### Factor Categorical variables


```{r, message=FALSE,warning=FALSE}

#remove default column, create unlabelled dataset and ID Column
data <- data[,!colnames(data) %in% c("default.payment.next.month")]


#factor categorical variables
factor_VARS <- c('SEX','EDUCATION','MARRIAGE','PAY_0','PAY_2','PAY_3','PAY_4','PAY_5','PAY_6')

data[factor_VARS]<- lapply(data[factor_VARS],function(x) as.factor(x))

str(data)

```


###Data Analysis
Now we explore the data. We can divide it into two categories:
  * Univariate Exploration
* Bivariate Exploration

####Univariate exploration
We can further divide this into two categories for data exploration:
  * Categorical Features
* Numerical Features

#####Categorical Features
Let us visualize the categorical data.

######SEX,EDUCATION,MARRIAGE
```{r, message=FALSE,warning=FALSE}
p1 = ggplot(data,aes(data$SEX))+geom_bar(fill="steelblue")+scale_x_discrete("Sex")+scale_y_continuous("No. of Observations")
p2 = ggplot(data,aes(data$EDUCATION))+geom_bar(fill="steelblue")+scale_x_discrete("Education")+scale_y_continuous("No. of Observations")
p3 = ggplot(data,aes(data$MARRIAGE))+geom_bar(fill="steelblue")+scale_x_discrete("Marriage")+scale_y_continuous("No. of Observations")
grid.arrange(p1,p2,p3,nrow = 1)
```

From the above graphs we made the following observations:
  1. The number of females are more in the dataset as compared to number of males.
2. There are some values at "0" for the attributes Education and Marriage.

######Pay Status
According to the description, PAY_x is a set of categorical variables with the levels:
  -1=pay duly, 1=payment delay for one month, 2=payment delay for two months,...8= payment delay for 8 months and 9=payment delay for 9 months and above.
```{r, message=FALSE,warning=FALSE}
p4 = ggplot(data,aes(data$PAY_0))+geom_bar(fill="steelblue")+scale_x_discrete("Payment Status for PAY_0")+scale_y_continuous("No. of Observations")
p5 = ggplot(data,aes(data$PAY_2))+geom_bar(fill="steelblue")+scale_x_discrete("Payment Status for PAY_2")+scale_y_continuous("No. of Observations")
p6 = ggplot(data,aes(data$PAY_3))+geom_bar(fill="steelblue")+scale_x_discrete("Payment Status for PAY_3")+scale_y_continuous("No. of Observations")
p7 = ggplot(data,aes(data$PAY_4))+geom_bar(fill="steelblue")+scale_x_discrete("Payment Status for PAY_4")+scale_y_continuous("No. of Observations")
p8 = ggplot(data,aes(data$PAY_5))+geom_bar(fill="steelblue")+scale_x_discrete("Payment Status for PAY_5")+scale_y_continuous("No. of Observations")
p9 = ggplot(data,aes(data$PAY_6))+geom_bar(fill="steelblue")+scale_x_discrete("Payment Status for PAY_6")+scale_y_continuous("No. of Observations")
grid.arrange(p4,p5,p6,p7,p8,p9,nrow = 2)
```

From the above graphs we made the following observation(s):
  We observed undocumented values for the PAY attributes i.e "0" &"-2"
On going through the discussions where the dataset was taken from we found that infact these values have the following meaning.
-2 = No consumption
0  = The use of revolving credit card.
So from here we found out that there are high number of observations where people are using revolving credit card.
Source:
  https://www.kaggle.com/uciml/default-of-credit-card-clients-dataset/discussion/34608 

#####Numerical Features

######Age
```{r, message=FALSE,warning=FALSE}
p10 = ggplot(data, aes(data$AGE)) + geom_histogram(binwidth = 1,colour="black",fill="white")+  scale_x_continuous("Age")+scale_y_continuous("Observations Count")+labs(title = "Histogram")
p11 = ggplot(data, aes(,data$AGE)) + geom_boxplot(fill = "white")+  scale_y_continuous("Age")+scale_x_continuous("")+labs(title="Boxplot")
grid.arrange(p10,p11,nrow =1,top="AGE DISTRIBUTION AND OUTLIERS" )
```

######Limit of Balance
Lets explore the distribution and the outliers for the limit of balance.
```{r, message=FALSE,warning=FALSE}
ggplot(data, aes(data$LIMIT_BAL)) + geom_histogram(binwidth = 5000,colour="black",fill="white")+  scale_x_continuous("Limit_BAL")+scale_y_continuous("Obs.")+labs(title = "Histogram")
```

So we can see from the above histogram the people usually have limit balance: 20000,30000,50000,80000,200000.

Now lets us check for outliers with a boxplot.
```{r, message=FALSE,warning=FALSE}
ggplot(data, aes(x=factor(0),y=data$LIMIT_BAL)) + geom_boxplot(fill = "white")+
  scale_y_continuous("Limit_Bal")+scale_x_discrete("")+labs(title="Boxplot")
```

So we can see the outliers are identified as individuals with a balance limit of over 600,000.
Let us see how many instances have values >600,000

```{r, message=FALSE,warning=FALSE}
LIMIT_BAL_OUT <- with(data, which(LIMIT_BAL > 600000))
NROW(LIMIT_BAL_OUT)
```

As we can see it is only 79 instances and we can consider removing these.

######Bill Amount
Lets explore the distribution and the outliers for BILL_AMTX. The document describes these attributes as follows:
  
  BILL_AMT1                  | Amount of bill statement in September, 2005 (NT dollar)
BILL_AMT2                  | Amount of bill statement in August, 2005 (NT dollar)
BILL_AMT3                  | Amount of bill statement in July, 2005 (NT dollar)
BILL_AMT4                  | Amount of bill statement in June, 2005 (NT dollar)
BILL_AMT5                  | Amount of bill statement in May, 2005 (NT dollar)
BILL_AMT6                  | Amount of bill statement in April, 2005 (NT dollar)

```{r, message=FALSE,warning=FALSE}
p12 = ggplot(data, aes(data$BILL_AMT1)) + geom_histogram(binwidth = 5000,colour="black",fill="white")+  scale_x_continuous("BILL_AMT1")+scale_y_continuous("Obs.")+labs(title = "Histogram")
p13 = ggplot(data, aes(data$BILL_AMT2)) + geom_histogram(binwidth = 5000,colour="black",fill="white")+  scale_x_continuous("BILL_AMT2")+scale_y_continuous("Obs.")+labs(title = "Histogram")
p14 = ggplot(data, aes(data$BILL_AMT3)) + geom_histogram(binwidth = 5000,colour="black",fill="white")+  scale_x_continuous("BILL_AMT3")+scale_y_continuous("Obs.")+labs(title = "Histogram")
p15 = ggplot(data, aes(data$BILL_AMT4)) + geom_histogram(binwidth = 5000,colour="black",fill="white")+  scale_x_continuous("BILL_AMT4")+scale_y_continuous("Obs.")+labs(title = "Histogram")
p16 = ggplot(data, aes(data$BILL_AMT5)) + geom_histogram(binwidth = 5000,colour="black",fill="white")+  scale_x_continuous("BILL_AMT5")+scale_y_continuous("Obs.")+labs(title = "Histogram")
p17 = ggplot(data, aes(data$BILL_AMT6)) + geom_histogram(binwidth = 5000,colour="black",fill="white")+  scale_x_continuous("BILL_AMT6")+scale_y_continuous("Obs.")+labs(title = "Histogram")
grid.arrange(p12,p13,p14,p15,p16,p17,nrow =3,top="BILL_AMTX DISTRIBUTION" )

```

Now we have seen the distribution, lets check for outliers:
```{r, message=FALSE,warning=FALSE}
p18 = ggplot(data, aes(,data$BILL_AMT1)) + geom_boxplot(fill = "white")+  scale_y_continuous("BILL_AMT1")+scale_x_continuous("")+labs(title="Boxplot")
p19 = ggplot(data, aes(,data$BILL_AMT2)) + geom_boxplot(fill = "white")+  scale_y_continuous("BILL_AMT2")+scale_x_continuous("")+labs(title="Boxplot")
p20 = ggplot(data, aes(,data$BILL_AMT3)) + geom_boxplot(fill = "white")+  scale_y_continuous("BILL_AMT3")+scale_x_continuous("")+labs(title="Boxplot")
p21 = ggplot(data, aes(,data$BILL_AMT4)) + geom_boxplot(fill = "white")+  scale_y_continuous("BILL_AMT4")+scale_x_continuous("")+labs(title="Boxplot")
p22 = ggplot(data, aes(,data$BILL_AMT5)) + geom_boxplot(fill = "white")+  scale_y_continuous("BILL_AMT5")+scale_x_continuous("")+labs(title="Boxplot")
p23 = ggplot(data, aes(,data$BILL_AMT6)) + geom_boxplot(fill = "white")+  scale_y_continuous("BILL_AMT6")+scale_x_continuous("")+labs(title="Boxplot")
grid.arrange(p18,p19,p20,p21,p22,p23,nrow =3,top="OUTLIERS for BILL_AMTX" )
```


######Pay Amount
Lets explore the distribution and the outliers for PAY_AMTX. The document describes these attributes as follows:
  
  PAY_AMT1                   | Amount of previous payment in September, 2005 (NT dollar)
PAY_AMT2                   | Amount of previous payment in August, 2005 (NT dollar)
PAY_AMT3                   | Amount of previous payment in July, 2005 (NT dollar)
PAY_AMT4                   | Amount of previous payment in June, 2005 (NT dollar)
PAY_AMT5                   | Amount of previous payment in May, 2005 (NT dollar)
PAY_AMT6                   | Amount of previous payment in April, 2005 (NT dollar)

```{r, message=FALSE,warning=FALSE}
p24 = ggplot(data, aes(data$PAY_AMT1)) + geom_histogram(binwidth = 5000,colour="black",fill="white")+  scale_x_continuous("PAY_AMT1")+scale_y_continuous("Obs.")+labs(title = "Histogram")
p25 = ggplot(data, aes(data$PAY_AMT2)) + geom_histogram(binwidth = 5000,colour="black",fill="white")+  scale_x_continuous("PAY_AMT2")+scale_y_continuous("Obs.")+labs(title = "Histogram")
p26 = ggplot(data, aes(data$PAY_AMT3)) + geom_histogram(binwidth = 5000,colour="black",fill="white")+  scale_x_continuous("PAY_AMT3")+scale_y_continuous("Obs.")+labs(title = "Histogram")
p27 = ggplot(data, aes(data$PAY_AMT4)) + geom_histogram(binwidth = 5000,colour="black",fill="white")+  scale_x_continuous("PAY_AMT4")+scale_y_continuous("Obs.")+labs(title = "Histogram")
p28 = ggplot(data, aes(data$PAY_AMT5)) + geom_histogram(binwidth = 5000,colour="black",fill="white")+  scale_x_continuous("PAY_AMT5")+scale_y_continuous("Obs.")+labs(title = "Histogram")
p29 = ggplot(data, aes(data$PAY_AMT6)) + geom_histogram(binwidth = 5000,colour="black",fill="white")+  scale_x_continuous("PAY_AMT6")+scale_y_continuous("Obs.")+labs(title = "Histogram")
grid.arrange(p24,p25,p26,p27,p28,p29,nrow =3,top="PAY_AMTX DISTRIBUTION" )

```

Now we have seen the distribution, lets check for outliers:
  
```{r, message=FALSE,warning=FALSE}
p30 = ggplot(data, aes(,data$PAY_AMT1)) + geom_boxplot(fill = "white")+  scale_y_continuous("PAY_AMT1")+scale_x_continuous("")+labs(title="Boxplot")
p31 = ggplot(data, aes(,data$PAY_AMT2)) + geom_boxplot(fill = "white")+  scale_y_continuous("PAY_AMT2")+scale_x_continuous("")+labs(title="Boxplot")
p32 = ggplot(data, aes(,data$PAY_AMT3)) + geom_boxplot(fill = "white")+  scale_y_continuous("PAY_AMT3")+scale_x_continuous("")+labs(title="Boxplot")
p33 = ggplot(data, aes(,data$PAY_AMT4)) + geom_boxplot(fill = "white")+  scale_y_continuous("PAYL_AMT4")+scale_x_continuous("")+labs(title="Boxplot")
p34 = ggplot(data, aes(,data$PAY_AMT5)) + geom_boxplot(fill = "white")+  scale_y_continuous("PAY_AMT5")+scale_x_continuous("")+labs(title="Boxplot")
p35 = ggplot(data, aes(,data$PAY_AMT6)) + geom_boxplot(fill = "white")+  scale_y_continuous("PAY_AMT6")+scale_x_continuous("")+labs(title="Boxplot")
grid.arrange(p30,p31,p32,p33,p34,p35,nrow =3,top="OUTLIERS for PAY_AMTX" )

```


####Bivariate Exploration

Let us now try to plot a relation between two attributes from the data set.

We will take the first two numerical attributes i.e Age and Limit_Bal and check if there is a trend.

```{r, message=FALSE,warning=FALSE}
ggplot(data, aes(AGE, LIMIT_BAL)) + geom_point() +
  scale_x_continuous("Age", breaks = seq(0,60,20))+
  scale_y_continuous("Limit_Bal", breaks = seq(0,1000000,by = 50000))+ theme_bw()

```


The relation b/w Age and Marriage should give us some kind of insight, lets check it out:
  
```{r, message=FALSE,warning=FALSE}
ggplot(data,aes(x=AGE,fill = MARRIAGE))+geom_bar(position = "fill")

```

As we saw earlier as well, there are some undocumented value for marriage "0".
Apart from that we see a clear trend in the dataset that as the age increases there are more number of married individuals.


Let us take a look at the relation between marriage and re-payment status:
  
```{r, message=FALSE,warning=FALSE}
ggplot(data,aes(x=PAY_0,fill = MARRIAGE))+geom_bar(position = "fill")
```
So it looks like the single people tend to delay the re-payment more than the married people do.

Now lets check if there is any trend we can observe if we plot the repayment status w.r.t sex.
We will start with PAY_0 i.e The repayment status in september just to see if we see a trend and then we can explore further.

```{r, message=FALSE,warning=FALSE}
ggplot(data,aes(x=PAY_0,fill = SEX))+geom_bar(position = "fill")
```

Looks like more number of females paid the amount duly in september and more number of males had a delay in re-payment.
Let us check for the rest of the months to see if it is the same situation every month.
```{r, message=FALSE,warning=FALSE}
b1 = ggplot(data,aes(x=PAY_2,fill = SEX))+geom_bar(position = "fill")
b2 = ggplot(data,aes(x=PAY_3,fill = SEX))+geom_bar(position = "fill")
b3 = ggplot(data,aes(x=PAY_4,fill = SEX))+geom_bar(position = "fill")
b4 = ggplot(data,aes(x=PAY_5,fill = SEX))+geom_bar(position = "fill")
b5 = ggplot(data,aes(x=PAY_6,fill = SEX))+geom_bar(position = "fill")
grid.arrange(b1,b2,b3,b4,b5,nrow = 3)
```

Just as we suspected! The males are more likely to delay the repayment as compared to the females.

# Modelling

## Select Modelling Technique

We chose to start with a k means clustering alorgirthm as most of our data was numeric and we were able to easily encode the categorical variables to make them numeric.  

## Encode Data

```{r, message=FALSE,warning=FALSE}
set.seed(456292)



#encod categorical columns
dummies_model <- dummyVars(ï..ID ~ ., data=data)

# Create the dummy variables using predict. The Y variable (ï..ID) will not be present in encod
encod <- predict(dummies_model, newdata = data)

# # Convert to dataframe
data_encoded <- data.frame(encod)

# # See the structure of the new dataset
str(data_encoded)


```


## Find Optimal number of Clusters (K)

To find the optimal number of clusters we used the Elbow method. The elbow method finds the optinal K value by considering the percentage of variance explained by each cluster.

```{r, message=FALSE,warning=FALSE}
#Find Optimal K using Elbow method

df <- sample_n(data_encoded,10000)

wssplot <- function(data, nc=15, seed=1234){
  wss <- (nrow(data)-1)*sum(apply(data,2,var))
  for (i in 2:nc){
    set.seed(seed)
    wss[i] <- sum(kmeans(data, centers=i)$withinss)}
  plot(1:nc, wss, type="b", xlab="Number of Clusters",
       ylab="Within groups sum of squares")}

wssplot(df, nc=10) 



```

Based on the Elbow method, we found the optinal number to be 6


## Build K Means Model 
```{r, message=FALSE,warning=FALSE}

kmeans.result <- kmeans(df, centers=6)

```

Explain K Means Models ......


## Assess Clusters

```{r, message=FALSE,warning=FALSE}


#Describe Clusters

#ClusterSize
kmeans.result$size

#Cluster Center Attributes 
kmeans.result$centers


#Vizualize Clusters
par(mfrow=c(3,2))
pie(colSums(df[kmeans.result$cluster==1,]),cex=0.5)

pie(colSums(df[kmeans.result$cluster==2,]),cex=0.5)

pie(colSums(df[kmeans.result$cluster==3,]),cex=0.5)
pie(colSums(df[kmeans.result$cluster==4,]),cex=0.5)
pie(colSums(df[kmeans.result$cluster==5,]),cex=0.5)
pie(colSums(df[kmeans.result$cluster==6,]),cex=0.5)

```


### Visualize Model with PCA

We will use Principle Componet Analysis (PCA) to help us Visualize our clusters.  PCA works by reducing dimensoinality of the data. We want to reduce the dimensionality to 2d because it will allow us to graph the clusters. PCA 
creates new columns that are linearly uncorrelated variables, and sorts thems in descending where PC1 will account for the highest variability of the data, and PC2 will be seocnd most and so on.

```{r, message=FALSE,warning=FALSE}
df_pca <- prcomp(df)
df_out <- as.data.frame(df_pca$x)

p<-ggplot(df_out,aes(x=PC1,y=PC2,color = as.factor(kmeans.result$cluster ) ))
theme<-theme(panel.background = element_blank(),panel.border=element_rect(fill=NA),panel.grid.major = element_blank(),panel.grid.minor = element_blank(),strip.background=element_blank(),axis.text.x=element_text(colour="black"),axis.text.y=element_text(colour="black"),axis.ticks=element_line(colour="black"),plot.margin=unit(c(1,1,1,1),"line"))
percentage <- round(df_pca$sdev / sum(df_pca$sdev) * 100, 2)
percentage <- paste( colnames(df_out), "(", paste( as.character(percentage), "%", ")", sep="") )

p<-p+geom_point()+theme+xlab(percentage[1]) + ylab(percentage[2])

p

```

Looking at the vizualized data we can see the clusters are well formed and there is minimal overlap, the dclusters are almost fully homogeneous.


##Find Anomalies (Outliers)



###Anomaly Detection using K Means

```{r, message=FALSE, warning=FALSE}

#Find the Center of each cluster
centers <- kmeans.result$centers[kmeans.result$cluster, ]  

# Calculate distance each point is from the center of the cluster
distances <- sqrt(rowSums((df - centers)^2))
# Take the top 20 fartherest points from the cluster center
outliers <- order(distances, decreasing=T)[1:20]

#Plot Outliers
p<-p + geom_point(data=df_out[outliers,],aes(x=PC1,y=PC2), colour="red", size=4)+ggtitle("PCA 6 Means Cluster with Outliers")

p



```



###Anomaly Detection using Local Outlier Factor


Another way to find outlier in unsupervised learning is by using the Local outlier
Facotr alogorithm.  This alogrithm is similar to the previous one we used but differs
by comparing local density,where locality is based on the k nearest neighbours.  The LOF
allows us to identify outliers in a data set tha would not be outliers in another area of the data set.


```{r, message=FALSE,warning=FALSE}

#run LOF Model takes awhile 5 to 10 minutes
outlier.scores <- lofactor(df,k=30)

#Plotdensity of Outliers
plot(density(outlier.scores))


#Plot outliers from LOF
outliers_lof <- order(outlier.scores, decreasing=T)[1:20]


p_lof<-ggplot(df_out,aes(x=PC1,y=PC2,color = as.factor(kmeans.result$cluster ) ))+geom_point()+theme+xlab(percentage[1]) + ylab(percentage[2]) + geom_point(data=df_out[outliers_lof,],aes(x=PC1,y=PC2), colour="red", size=3)+ggtitle("PCA LOF Outliers")


#compare K means outlier to LOF
grid.arrange(p, p_lof, nrow = 2,ncol = 1)



```







### Look at individual outliers

```{r, message=FALSE,warning=FALSE}


```

The Random Forest with 500 trees and mtry of`r bestmtry$mtry` had a overall accuracy of  `r round(RF_confus$overall[1]*100,2)` compared to the decision tree model of `r round(confus$overall[1]*100,2)`.  Like the decision tree model the random forest is better at accurately classify target values of 1.  In the above graph the green line shows the error on the TARGET value of 1, the red line shows the error of classify Target value of 0 and the black line shows the Out of bag error of the random forest.  From the chart it looks like the error has converged at around 200 trees, but it does look like it is starting to coverge again around 400 trees. What would happen if we were to add more trees?

### Find Optimal Tree

```{r, message=FALSE,warning=FALSE,tidy=TRUE}
control <- trainControl(method="repeatedcv", number=10, repeats=3, search="grid")

modellist <- list()
# loop to iterate through random forest models with different number of trees
for (ntree in c(200,1000, 1500, 2000)) {
fit <- train(factor(TARGET)~., data=train_data, method="rf", metric="Accuracy", tuneGrid=tunegrid,tuneLength = 50, trControl=control, ntree=ntree)
key <- toString(ntree)
modellist[[key]] <- fit
}
# compare results of all models (1000,1500 and 2000 trees)
results <- resamples(modellist)
summary(results)

# Predict Test results 100 Trees
RF200_model.pred <- predict(modellist$`200`, test_data)

# Check Accuracy of Model 
RF200_confus <- confusionMatrix(factor(RF200_model.pred),factor(test_data$TARGET))

# Predict Test results 1000 Trees
RF1000_model.pred <- predict(modellist$`1000`, test_data)

# Check Accuracy of Model 
RF1000_confus <- confusionMatrix(factor(RF1000_model.pred),factor(test_data$TARGET))

# Predict Test results 1500 Trees
RF2000_model.pred <- predict(modellist$`2000`, test_data)

# Check Accuracy of Model 
RF2000_confus <- confusionMatrix(factor(RF2000_model.pred),factor(test_data$TARGET))

# Predict Test results 2000 Trees
RF1500_model.pred <- predict(modellist$`1500`, test_data)

# Check Accuracy of Model 
RF1500_confus <- confusionMatrix(factor(RF1500_model.pred),factor(test_data$TARGET))

#Compare accuracy across all models created
pred_table <- data.table(Model = c("Decision Tree","Random Forest 200 Trees","Random Forest 500 Trees","Random Forest 1000 Tress","Random Forest 1500 Trees","Random Forest 2000 Trees"),Accuracy = c(confus$overall[1],RF200_confus$overall[1],RF_confus$overall[1],RF1000_confus$overall[1],RF1500_confus$overall[1],RF2000_confus$overall[1]))

pred_table

```

We ran the Random forest (RF) model with 200, 1000, 1500 and 2000 trees and compared there accuracy to our RF model with 500 trees and the Decision tree model. We can see the best model was the random forest with 1500 trees. This had an accuracy of `r round(RF1500_confus$overall[1]*100,2)`


# Evaluation

In summary, we have tried various models and tuning parameters and below summarizes the prediction accuracies:

Model         | Prediction Accuracy
--------------|---------------------
Decision Tree | 73.8%
RF 200        | 80.8%
RF 500        | 80.3%
RF 1000       | 80.3%
RF 1500       | 80.9%
RF 2000       | 80.3%

From the results above, we have the following observations:

* The random forest model is superior to the decision tree model as it is an ensemble model.
* For random forest, the errors converge fairly quickly starting at 100 trees. So the accuracy didn't improve further with the number of trees greater than 100. 

We reckon that 80% of prediction accuracy would provide a good foundation for further studies with the goal to use the model for clinical studies.

Futhermore, we found that the top 3 important measurements which could determine the heart disease diagnosis is as follows:

* THALACH (Maximum Heart Rate Achieved)
* EXANG (Exercise Induced Angina)
* OLDPEAK (ST Depression Induced by Exercise Relative to Rest)

Based on these findings, we will recommend Peter Munk to allocate more budget to improve the facilities and apparatus which are used to measure these attributes.

##Next Steps

The next step would be to try and acquire more data to train the model on.  Another step would be to try other classification models that do not use decision trees.  We could try Support Vector machines or Logistic Regression.


# Deployment

Given we have a model with fairly accurate prediction, we will deploy the model for the doctors to perform trial runs and receive feedback. 
