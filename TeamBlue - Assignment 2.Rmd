---
title: "Clustering Algorithm for Unupervised Learning - Credit Card Client Anomaly Analysis"
author: "Tyler Blakeley, Benjamin Kan, Mohammad Islam, Avijeet Singh"
date: "October 29 2018"
output:
  html_document:
    fig_height: 4.5
    fig_width: 7
    highlight: tango
    number_sections: yes
    theme: readable
    toc: yes
  pdf_document:
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE)
```
# Business Understanding
We work at the Retail Credit Risk Analytics department at the Bank of Taiwan. Recently, there have been increasing credit card debt defaults in our bank. Senior Management would like our department to develop a machine learning algorithm to find anomalies in the data that we hope will show early warning signs of default. This will allow the Retail Credit Risk and Collections departments to act early by reducing these cleints' credit card limits to minimize the losses. We would also like to find out which demographics are in the anomaly group which would indicate high susceptiblility of defaults. The Management instructed us to use data from the third parties to build the algorithms as proof-of-concepts before we use our own data. They would also like us to build a user-friendly app to allow them to load in the dataset and identify clients that are in the anomaly group which may indicate high risk of defaulting on their credit card debts.        

# Data Understanding

## Data Source and Collection
We sourced the third party credit card data from Kaggle (https://www.kaggle.com/uciml/default-of-credit-card-clients-dataset). This dataset contains information on default payments, demographic factors, credit data, history of payment, and bill statements of credit card clients in Taiwan from April 2005 to September 2005. As mentioned above, the goal is to identify a group of customers who exhibit abnormal behaviour comparing with the rest of the data. We assume that abnormal credit behaviour would lead to high default risks.

## Data Description
In the dataset, there are 25 variables:

Variable Name              | Description
---------------------------|--------------------------------------------------------------------------------
ID                         | ID of each client
LIMIT_BAL                  | Amount of given credit in NT dollars (includes individual and                                               | family/supplementary credit
SEX                        | Gender (1=male, 2=female)
EDUCATION                  | (1=graduate school, 2=university, 3=high school, 4=others, 5=unknown, 6=unknown)
MARRIAGE                   | Marital status (1=married, 2=single, 3=others)
AGE                        | Age in years
PAY_0                      | Repayment status in September, 2005 (-1=pay duly, 1=payment delay for one month,                            | 2=payment delay for two months, ... 8=payment delay for eight months, 9=payment                             | delay for nine months and above)                 
PAY_2                      | Repayment status in August, 2005 (scale same as above)
PAY_3                      | Repayment status in July, 2005 (scale same as above)
PAY_4                      | Repayment status in June, 2005 (scale same as above)
PAY_5                      | Repayment status in May, 2005 (scale same as above)
PAY_6                      | Repayment status in April, 2005 (scale same as above)
BILL_AMT1                  | Amount of bill statement in September, 2005 (NT dollar)
BILL_AMT2                  | Amount of bill statement in August, 2005 (NT dollar)
BILL_AMT3                  | Amount of bill statement in July, 2005 (NT dollar)
BILL_AMT4                  | Amount of bill statement in June, 2005 (NT dollar)
BILL_AMT5                  | Amount of bill statement in May, 2005 (NT dollar)
BILL_AMT6                  | Amount of bill statement in April, 2005 (NT dollar)
PAY_AMT1                   | Amount of previous payment in September, 2005 (NT dollar)
PAY_AMT2                   | Amount of previous payment in August, 2005 (NT dollar)
PAY_AMT3                   | Amount of previous payment in July, 2005 (NT dollar)
PAY_AMT4                   | Amount of previous payment in June, 2005 (NT dollar)
PAY_AMT5                   | Amount of previous payment in May, 2005 (NT dollar)
PAY_AMT6                   | Amount of previous payment in April, 2005 (NT dollar)
default.payment.next.month | Default payment (1=yes, 0=no)

## Data Exploration

### Load Packages
```{r, message=FALSE,warning=FALSE}
#import packages;
library(dplyr)
library(reshape2)
library(ggplot2)
library(Hmisc)
library(corrplot)
library(mice)
library(VIM)
library(pROC)
library(caret)
library(corrgram)
library(GGally)
library(ggthemes) 
library(DMwR)
library(gridExtra)
library(rattle)
library(readxl)
library(cluster)
library(sqldf)
```
### Load Datasets
Now that the packages are loaded, we can load in the dataset.

```{r, message=FALSE}
data <- read.csv("default of credit card clients.csv", header = TRUE, na= 'NA')
```

Now lets look at the structure of the data.

```{r, message=FALSE}
str(data)
```

### Renaming the values in categorical attributes

We note that the attribute "SEX" has numeric values. We will change the numeric values to M & F so that it is easier to read while plotting.

```{r, message=FALSE,warning=FALSE}
data$SEX<-ifelse(data$SEX==1,"M","F")
```


###  Create Unlabelled Dataset and Factor Categorical Variables

The dataset that we have has labelled data as we have the target value in the last column. In order to create a unlabelled dataset for our report we decided to remove this column.

```{r, message=FALSE,warning=FALSE}
data <- data[,!colnames(data) %in% c("default.payment.next.month")]
```

Now we will factorize some of the categorical attributes from the dataset.

```{r, message=FALSE,warning=FALSE}
factor_VARS <- c('SEX','EDUCATION','MARRIAGE','PAY_0','PAY_2','PAY_3','PAY_4','PAY_5','PAY_6')

data[factor_VARS]<- lapply(data[factor_VARS],function(x) as.factor(x))
```

Now let us check the data structure

```{r, message=FALSE,warning=FALSE}
str(data)
```

Now we have appropriate variables that are converted into factors and the last column has been removed to make the dataset unlabelled. 

###Data Analysis
Now we explore the data. We can divide it into two categories:
* Univariate Exploration
* Bivariate Exploration

####Univariate exploration
We can further divide this into two categories for data exploration:
* Categorical Features
* Numerical Features

#####Categorical Features
Let us visualize the categorical data.

######SEX,EDUCATION,MARRIAGE
```{r, message=FALSE,warning=FALSE}
p1 = ggplot(data,aes(data$SEX))+geom_bar(fill="steelblue")+scale_x_discrete("Sex")+scale_y_continuous("No. of Observations")
p2 = ggplot(data,aes(data$EDUCATION))+geom_bar(fill="steelblue")+scale_x_discrete("Education")+scale_y_continuous("No. of Observations")
p3 = ggplot(data,aes(data$MARRIAGE))+geom_bar(fill="steelblue")+scale_x_discrete("Marriage")+scale_y_continuous("No. of Observations")
grid.arrange(p1,p2,p3,nrow = 1)
```

From the above graphs we made the following observations:
1. The number of females are more in the dataset as compared to number of males.
2. There are some values at "0" for the attributes Education and Marriage.
3. For education level, 5 and 6 are categorized as unknown so we can look into grouping them into one level.

######Payment Status
According to the description, PAY_x is a set of categorical variables with the levels:
 -1   = pay duly
  1   = payment delay for one month
  2   = payment delay for two months,
..8   = payment delay for 8 months and 9=payment delay for 9 months and above.

```{r, message=FALSE,warning=FALSE}
p4 = ggplot(data,aes(data$PAY_0))+geom_bar(fill="steelblue")+scale_x_discrete("Payment Status Sept_2005")+scale_y_continuous("No. of Observations")
p5 = ggplot(data,aes(data$PAY_2))+geom_bar(fill="steelblue")+scale_x_discrete("Payment Status Aug_2005")+scale_y_continuous("No. of Observations")
p6 = ggplot(data,aes(data$PAY_3))+geom_bar(fill="steelblue")+scale_x_discrete("Payment Status July_2005")+scale_y_continuous("No. of Observations")
p7 = ggplot(data,aes(data$PAY_4))+geom_bar(fill="steelblue")+scale_x_discrete("Payment Status June_2005")+scale_y_continuous("No. of Observations")
p8 = ggplot(data,aes(data$PAY_5))+geom_bar(fill="steelblue")+scale_x_discrete("Payment Status May_2005")+scale_y_continuous("No. of Observations")
p9 = ggplot(data,aes(data$PAY_6))+geom_bar(fill="steelblue")+scale_x_discrete("Payment Status April_2005")+scale_y_continuous("No. of Observations")
grid.arrange(p4,p5,p6,p7,p8,p9,nrow = 2)
```

From the above graphs we made the following observation(s):
We observed undocumented values for the PAY attributes i.e "0" &"-2"
We went back to Kaggle and searched for related discussions in the forum and we found that these values have the following meaning.
* -2 = No consumption
* 0  = The use of revolving credit card.
Source: https://www.kaggle.com/uciml/default-of-credit-card-clients-dataset/discussion/34608 

So from here we found out that there are high number of observations where people are using revolving credit card. We also observed that there is a spike in payment delays in September 2005.

#####Numerical Features

We would like to know the distribution of the numerical variables:

######Age
```{r, message=FALSE,warning=FALSE}
p10 = ggplot(data, aes(data$AGE)) + geom_histogram(binwidth = 1,colour="black",fill="white")+  scale_x_continuous("Age")+scale_y_continuous("Observations Count")+labs(title = "Histogram")
p11 = ggplot(data, aes(,data$AGE)) + geom_boxplot(fill = "white")+  scale_y_continuous("Age")+scale_x_continuous("")+labs(title="Boxplot")
grid.arrange(p10,p11,nrow =1,top="AGE DISTRIBUTION AND OUTLIERS" )
```

As we can see above, the use of credit cards concentrates on younger age (< 40) with the average age of `r mean(data$AGE)`. 

######Limit of Balance
Lets explore the distribution and the outliers for the limit of balance.
```{r, message=FALSE,warning=FALSE}
ggplot(data, aes(data$LIMIT_BAL)) + geom_histogram(binwidth = 5000,colour="black",fill="white")+  scale_x_continuous("Limit_BAL")+scale_y_continuous("Obs.")+labs(title = "Histogram")
```

So we can see from the above histogram the people usually have the following limit balances: 20K, 30K, 50K, 80K, 200K and 500K.

Now lets us check for outliers with a boxplot.

```{r, message=FALSE,warning=FALSE}
ggplot(data, aes(x=factor(0),y=data$LIMIT_BAL)) + geom_boxplot(fill = "white")+
  scale_y_continuous("Limit_Bal")+scale_x_discrete("")+labs(title="Boxplot")
```

So we can see that the 75th percentile has the credit limit of 250K. Next we would like to know how many customers have extremely high credit limits. We set 600K as the threshold based on the boxplot above.


```{r, message=FALSE,warning=FALSE}
LIMIT_BAL_OUT <- with(data, which(LIMIT_BAL > 600000))
NROW(LIMIT_BAL_OUT)
```

As we can see, it is only 79 instances and we can consider removing these.

######Average Billing Amount & Payment Amount.
Now we will first take the averages of the billing amount and payment amount for each observations in order to plot a distribution.

```{r, message=FALSE,warning=FALSE}
data$billmean <- rowMeans(data[c('BILL_AMT1', 'BILL_AMT2','BILL_AMT3','BILL_AMT4','BILL_AMT5','BILL_AMT6')], na.rm=TRUE)
data$paymean<-rowMeans(data[c('PAY_AMT1', 'PAY_AMT2','PAY_AMT3','PAY_AMT4','PAY_AMT5','PAY_AMT6')], na.rm=TRUE)
```

Now that we have introduced the mean attributes let us plot for the same.

```{r, message=FALSE,warning=FALSE}
p12 = ggplot(data, aes(data$billmean)) + geom_histogram(binwidth = 5000,colour="black",fill="white")+  scale_x_continuous("Mean Bill Amount")+scale_y_continuous("Obs.")
p24 = ggplot(data, aes(data$paymean)) + geom_histogram(binwidth = 5000,colour="black",fill="white")+  scale_x_continuous("Mean Payment Amount")+scale_y_continuous("Obs.")
grid.arrange(p12,p24,nrow=1,top = "Avergae Bill & Payment amount for all months")
```

From 
Now we have seen the distribution, lets check for outliers:

```{r, message=FALSE,warning=FALSE}
p18 = ggplot(data, aes(,data$billmean)) + geom_boxplot(fill = "white")+ scale_y_continuous("Average Billing Amount")+scale_x_continuous("")
p30 = ggplot(data, aes(,data$paymean)) + geom_boxplot(fill = "white")+  scale_y_continuous("Average Payment Amount")+scale_x_continuous("")
grid.arrange(p18,p30,ncol=2,top="Boxplot for Avergae Bill & Payment amount for all months")
```


####Bivariate Exploration
We will look at the following relationships in the dataset:

* Marriage Status and Repayment Status
* Sex and repayent Status
* Education and repayment Status


According to the graph above, there is little relationship between age and credit limits.

#####Marriage status and Repayment Status

Let us take a look at the relationship between marriage status and re-payment status as of September 2005:
  
```{r, message=FALSE,warning=FALSE}
ggplot(data,aes(x=PAY_0,fill = MARRIAGE))+geom_bar(position = "fill")+scale_x_discrete("Repayment Status in September 2005")
```
So it looks like the single people tend to delay the re-payment more than the married people do, with some exceptions.

#####Sex and Repayment Status

Now lets check if there is any trend we can observe if we plot the repayment status w.r.t sex.
We will start with PAY_0 i.e The repayment status in september just to see if we see a trend and then we can explore further.

```{r, message=FALSE,warning=FALSE}
ggplot(data,aes(x=PAY_0,fill = SEX))+geom_bar(position = "fill")+scale_x_discrete("Repayment Status in September 2005")
```

Looks like more number of females paid the amount duly in september and more number of males had a delay in re-payment.
Let us check for the rest of the months to see if it is the same situation every month.

```{r, message=FALSE,warning=FALSE}
b1 = ggplot(data,aes(x=PAY_2,fill = SEX))+geom_bar(position = "fill")+scale_x_discrete("Repayment Status in August 2005")
b2 = ggplot(data,aes(x=PAY_3,fill = SEX))+geom_bar(position = "fill")+scale_x_discrete("Repayment Status in July 2005")
b3 = ggplot(data,aes(x=PAY_4,fill = SEX))+geom_bar(position = "fill")+scale_x_discrete("Repayment Status in June 2005")
b4 = ggplot(data,aes(x=PAY_5,fill = SEX))+geom_bar(position = "fill")+scale_x_discrete("Repayment Status in May 2005")
b5 = ggplot(data,aes(x=PAY_6,fill = SEX))+geom_bar(position = "fill")+scale_x_discrete("Repayment Status in April 2005")
grid.arrange(b1,b2,b3,b4,b5,nrow = 3)
```

Just as we suspected! The males are more likely to delay the repayment as compared to the females.

#####Education and Repayment Status

```{r, message=FALSE,warning=FALSE}
ggplot(data,aes(x=PAY_0,fill = EDUCATION))+geom_bar(position = "fill")+scale_x_discrete("Repayment Status in September 2005")
```

##DATA PREPARATION

###Check for missing values.
Now that we have explored the original data lets look for missing values.
```{r, message=FALSE,warning=FALSE}
sum(is.na.data.frame(data))
```
So we can see from above that we have 0 missing values so we dont have to worry about that.

###Data Cleaning
Earlier we had introduced two ne attributes(billmean & paymean) for our data explorations, we will now remove them as we dont need them in our modelling.
```{r, message=FALSE,warning=FALSE}
data$billmean<-NULL
data$paymean<-NULL
```


## Modelling

### Select Modelling Technique

We chose to start with a k means clustering alorgirthm as most of our data was numeric. The first step to running the alogirthm is to make all the categorical variables into numeric by encoding them.  We chose to implement one hot encoding because our variables are nominal and not ordinal. 

### Encode Data

```{r, message=FALSE,warning=FALSE}
set.seed(456292)



#encod categorical columns
dummies_model <- dummyVars(ï..ID ~ ., data=data)

# Create the dummy variables using predict. The Y variable (ï..ID) will not be present in encod
encod <- predict(dummies_model, newdata = data)

# # Convert to dataframe
data_encoded <- data.frame(encod)

# # Summary of the new dataset
str(data_encoded)




```

Now that the data is encoded and all variables are numeric we can implement the K means algorithm.  The K means algorithm works by creating K cluster.  Each cluster is based on feature similarity.  The alogirthm starts by randomly picking K centroids, then the alogirthm iterates through each point and assigns it to the nearest cluster based a distance formula. (ex: Euclidean).  After each point as been assigned a cluster, the centroids are re calculated by taking the mean of all data points in the clusters, next the points are reclassifed again until a stopping criteria is met. (No points change clusters or the mean of all data points in the cluster is minimized.)


### Scale the Data

Since the K means algorithm using distance to find the nearest point we need to ensure all the variables are on the same range.  We do this by normalizing the data.

```{r, message=FALSE,warning=FALSE}
df <- data_encoded

normalize = function(x) {
  return ((x - min(x)) / (max(x) - min(x)))
}

df = as.data.frame(lapply(df, normalize))

summary(df)
```

### Find Optimal number of Clusters (K)

Before we run the K Mean algorithm we need to find the optimal K clusters. To find the optimal number of clusters we used the Elbow method. The elbow method finds the optinal K value by running mulptiple K Means for K from 1 to N and calculateing the Sum of Squared errors (SSE).  We then plot each SSE for each K, we then chose the smallest K such that it has a low SSE which is at the elbow point of the plot.

```{r, message=FALSE,warning=FALSE}
#Find Optimal K using Elbow method

df <- sample_n(data_encoded,30000)

wssplot <- function(data, nc=15, seed=1234){
  wss <- (nrow(data)-1)*sum(apply(data,2,var))
  for (i in 2:nc){
    set.seed(seed)
    wss[i] <- sum(kmeans(data, centers=i, nstart=5)$withinss)}
  plot(1:nc, wss, type="b", xlab="Number of Clusters",
       ylab="Within groups sum of squares")}

wssplot(df, nc=10) 



```

Based on the Elbow method, we found the optinal number to be 6. Beyond 6, we see minimal improvement.


### Build K Means Model 

We are now able to create the K mean model, we are creating K = 6 clusters, we also use nstart = 5 to run the k means algorithm 5 times with random centroids each time and take the best result.

```{r, message=FALSE,warning=FALSE}

#set.seed(1234)
set.seed(9897665)

kmeans.result <- kmeans(df, centers=6,nstart = 30)

```

Explain K Means Models ......

## Describe Cluster


### Assess Clusters


```{r, message=FALSE,warning=FALSE}


#Describe Clusters

#ClusterSize
kmeans.result$size

#Cluster Center Attributes 
kmeans.result$centers



```

### Numeric Variables


```{r, message=FALSE,warning=FALSE}

#Create 2 data sets one with Categorical and one with Numeric variables
nn <- data[,!colnames(data) %in% c('?..ID','SEX','EDUCATION','MARRIAGE','PAY_0','PAY_2','PAY_3','PAY_4','PAY_5','PAY_6','ï..ID')]
cc <- data[,colnames(data) %in% c('SEX','EDUCATION','MARRIAGE','PAY_0','PAY_2','PAY_3','PAY_4','PAY_5','PAY_6','ï..ID','AGE')]

#Add in Cluster results 
nn$Cluster <- kmeans.result$cluster
cc$Cluster <- kmeans.result$cluster

#Transpose data 
mn <- melt(nn, id.vars = c("Cluster"))

mc <- melt(cc, id.vars = c("Cluster"))

#Aggregate Average value for each cluster and numeric variable
a_mn <- sqldf('SELECT Cluster, Variable, avg(value) as AvgValue
FROM mn
GROUP BY Cluster, Variable')


#Factor Cluster varaible to be discrete
a_mn$Cluster <- as.factor(a_mn$Cluster)

#Plot clusters and variables
x <- ggplot(data = a_mn, aes(x = variable, y = AvgValue, group = Cluster, fill = Cluster))
x <- x + geom_bar(stat = "identity", width = 0.5, position = "dodge")
x <- x + theme_bw()
x <- x +  coord_flip()



#Crosstab Data

reshape2::dcast(a_mn, Cluster ~ variable, value.var = "AvgValue")

x

```

For each cluster we calculated the average value for each numeric variable.  From these average we are able to describe the clusters. We noticed that age is very similar for all clusters.  Cluster 1 has the highest average Limit Balance and the points also have the highest Bill Amount,but they dont always have the highest Payment amount.  Cluster 2 has the lowest average Bill Amount and the second lowest Limit Balance.  Cluster 6 has the Lowest Limit Balance but they have the second highest Bill Amount, they are also the third lowest in average payment.

Cluster 1 - 
Cluster 2 -
Cluster 3-
Cluster 4- High Balance / High Bill Amount / Pays Significant Amount 
Cluster 5-
Cluster 6-


### Categorical Variables

```{r, message=FALSE,warning=FALSE}


#Aggregate Freqeuncy for each cluster and categorical variable
a_mc <- sqldf("SELECT Cluster, Variable,Value, COUNT(*) as Freq
FROM mc
GROUP BY Cluster, Variable,Value")

#Factor Cluster varaible to be discrete
a_mc$Cluster <- as.factor(a_mc$Cluster)

#Plot clusters and variables
edu <- ggplot(data = a_mc[a_mc$variable =="EDUCATION",], aes(x = Cluster, y = Freq, group = variable, fill = value)) + geom_bar(stat = "identity", width = 0.5, position = "dodge")

sex <- ggplot(data = a_mc[a_mc$variable =="SEX",], aes(x = Cluster, y = Freq, group = variable, fill = value)) + geom_bar(stat = "identity", width = 0.5, position = "dodge")

mar <- ggplot(data = a_mc[a_mc$variable =="MARRIAGE",], aes(x = Cluster, y = Freq, group = variable, fill = value)) + geom_bar(stat = "identity", width = 0.5, position = "dodge")


pay <- ggplot(data = a_mc[a_mc$variable=='PAY_0'|a_mc$variable=='PAY_2'|a_mc$variable=='PAY_3'|a_mc$variable=='PAY_4'|a_mc$variable=='PAY_5'|a_mc$variable=='PAY_6',], aes(x = Cluster, y = Freq, group = variable, fill = value))

pay <- pay + geom_bar(stat = "identity", width = 0.5, position = "fill") + facet_grid(. ~ variable)  + labs(title = "Paided On Time: Cluster Analysis")
pay <- pay +  coord_flip() + theme(axis.title.x=element_blank(),
axis.text.x=element_blank(),
axis.ticks.x=element_blank())




grid.arrange(edu,sex,mar,nrow = 3)
pay


```


#### Visualize Model with PCA

We will use Principle Componet Analysis (PCA) to help us Visualize our clusters.  PCA works by reducing dimensoinality of the data. We want to reduce the dimensionality to 2d because it will allow us to graph the clusters. PCA 
creates new columns that are linearly uncorrelated variables, and sorts thems in descending where PC1 will account for the highest variability of the data, and PC2 will be seocnd most and so on.

```{r, message=FALSE,warning=FALSE}
df_pca <- prcomp(df)
df_out <- as.data.frame(df_pca$x)

p<-ggplot(df_out,aes(x=PC1,y=PC2,color = as.factor(kmeans.result$cluster ) ))
theme<-theme(panel.background = element_blank(),panel.border=element_rect(fill=NA),panel.grid.major = element_blank(),panel.grid.minor = element_blank(),strip.background=element_blank(),axis.text.x=element_text(colour="black"),axis.text.y=element_text(colour="black"),axis.ticks=element_line(colour="black"),plot.margin=unit(c(1,1,1,1),"line"))
percentage <- round(df_pca$sdev / sum(df_pca$sdev) * 100, 2)
percentage <- paste( colnames(df_out), "(", paste( as.character(percentage), "%", ")", sep="") )

p<-p+geom_point()+theme+xlab(percentage[1]) + ylab(percentage[2])

p

```

Looking at the vizualized data we can see the clusters are well formed and there is minimal overlap, the dclusters are almost fully homogeneous.


###Find Anomalies (Outliers)



####Anomaly Detection using K Means

```{r, message=FALSE, warning=FALSE}

#Find the Center of each cluster
centers <- kmeans.result$centers[kmeans.result$cluster, ]  

# Calculate distance each point is from the center of the cluster
distances <- sqrt(rowSums((df - centers)^2))
# Take the top 20 fartherest points from the cluster center
outliers <- order(distances, decreasing=T)[1:20]

#Plot Outliers
p<-p + geom_point(data=df_out[outliers,],aes(x=PC1,y=PC2), colour="red", size=4)+ggtitle("PCA 6 Means Cluster with Outliers")

p



```



####Anomaly Detection using Local Outlier Factor


Another way to find outlier in unsupervised learning is by using the Local outlier
Facotr alogorithm.  This alogrithm is similar to the previous one we used but differs
by comparing local density,where locality is based on the k nearest neighbours.  The LOF
allows us to identify outliers in a data set tha would not be outliers in another area of the data set.


```{r, message=FALSE,warning=FALSE}

#run LOF Model takes awhile 5 to 10 minutes
outlier.scores <- lofactor(df,k=15)

#Plotdensity of Outliers
plot(density(outlier.scores))


#Plot outliers from LOF
outliers_lof <- order(outlier.scores, decreasing=T)[1:20]


p_lof<-ggplot(df_out,aes(x=PC1,y=PC2,color = as.factor(kmeans.result$cluster ) ))+geom_point()+theme+xlab(percentage[1]) + ylab(percentage[2]) + geom_point(data=df_out[outliers_lof,],aes(x=PC1,y=PC2), colour="red", size=3)+ggtitle("PCA LOF Outliers")


#compare K means outlier to LOF
grid.arrange(p, p_lof, nrow = 2,ncol = 1)



```







#### Look at individual outliers

```{r, message=FALSE,warning=FALSE}


#Add cluster column to data
data_out <- data
data_out$cluster <- kmeans.result$cluster

km_outlier <- data_out[outliers,]



table(km_outlier$cluster)


km_a

data[outliers_lof]
```

## Evaluation

In summary, we have tried various models and tuning parameters and below summarizes the prediction accuracies:

Model         | Prediction Accuracy
--------------|---------------------
Decision Tree | 73.8%
RF 200        | 80.8%
RF 500        | 80.3%
RF 1000       | 80.3%
RF 1500       | 80.9%
RF 2000       | 80.3%

From the results above, we have the following observations:

* The random forest model is superior to the decision tree model as it is an ensemble model.
* For random forest, the errors converge fairly quickly starting at 100 trees. So the accuracy didn't improve further with the number of trees greater than 100. 

We reckon that 80% of prediction accuracy would provide a good foundation for further studies with the goal to use the model for clinical studies.

Futhermore, we found that the top 3 important measurements which could determine the heart disease diagnosis is as follows:

* THALACH (Maximum Heart Rate Achieved)
* EXANG (Exercise Induced Angina)
* OLDPEAK (ST Depression Induced by Exercise Relative to Rest)

Based on these findings, we will recommend Peter Munk to allocate more budget to improve the facilities and apparatus which are used to measure these attributes.

##Next Steps

The next step would be to try and acquire more data to train the model on.  Another step would be to try other classification models that do not use decision trees.  We could try Support Vector machines or Logistic Regression.


## Deployment

Given we have a model with fairly accurate prediction, we will deploy the model for the doctors to perform trial runs and receive feedback. 
